{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --force-reinstall numpy\n",
        "!pip install bertopic gensim scikit-learn sentence-transformers tqdm openpyxl"
      ],
      "metadata": {
        "id": "d4SnBjl1cKjw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from bertopic import BERTopic\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from gensim.models import CoherenceModel\n",
        "from gensim.corpora import Dictionary\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from google.colab import files"
      ],
      "metadata": {
        "id": "SV6TyZwEbQAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "import io\n",
        "import pandas as pd\n",
        "\n",
        "for filename in uploaded:\n",
        "    if filename.endswith(\".xlsx\"):\n",
        "        df = pd.read_excel(io.BytesIO(uploaded[filename]))\n",
        "    elif filename.endswith(\".csv\"):\n",
        "        df = pd.read_csv(io.BytesIO(uploaded[filename]))\n",
        "    else:\n",
        "        raise ValueError(\"지원되지 않는 파일 형식입니다. .xlsx 또는 .csv를 사용하세요.\")"
      ],
      "metadata": {
        "id": "g8VZE5MBbR6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords = set([\n",
        "    '그', '이', '저', '것', '등', '때', '중', '누구', '무엇', '하다', '되다', '있다', '없다', '받다',\n",
        "    '가다', '오다', '보다', '주다', '말하다', '들다', '계속', '이제', '지금', '예전', '요즘', '거의',\n",
        "    '좀', '많이', '더', '다시', '별로', '그리고', '그래서', '그런데', '하지만', '그러니까', '때문',\n",
        "    '이런', '그런', '어떤', '같다', '나', '내', '너', '우리', '당신', '그녀', '그들', '사람', '모두',\n",
        "    '여기', '저기', '거기', '안', '밖', '위', '아래', '그곳', '방향', '도', '는', '만', '과', '와',\n",
        "    '보다', '까지', '부터', '으로', '에게', '이랑', '밖에', '조차', '정도', '경우', '내용', '문제',\n",
        "    '이야기', '모습', '상황', '자신', '음', '응', '오', '어휴', '허', '아이고', '에휴', '웃음',\n",
        "    '그떄', '그다음', '아마', '...', '…', '“', '”', '‘', '’', '-', '--', '―', '그냥', '진짜', '완전',\n",
        "    '약간', '어떻게', '뭐', '딱', '막', '또', '또는', '해도', '생각', '중략', '가지', '조금',\n",
        "    '다문화', '중국', '일본', '한국', '우즈베키스탄', '베트남', '태국', '몽골', '몽고',\n",
        "    '자기', '그때', '그거', '수도', '그게', '여러', '무슨', '네네', '나이', '어디', '먼저', '대부분',\n",
        "    '나중', '대해', '그것', '뭔가', '전혀', '저희', '만약', '이주', '나가야', '다른', '항상',\n",
        "    '얘기',\n",
        "])\n",
        "\n",
        "standardization_dict = {\n",
        "    \"아빠\": \"남편\", \"여자\": \"여성\", \"아기\": \"아이\", \"한국말\": \"한국어\",\n",
        "    \"시엄마\": \"시어머니\", \"어머니\": \"시어머니\", \"대한민국\": \"한국\",\n",
        "    \"외국\": \"외국인\", \"외국사람\": \"외국인\", \"차별적\": \"차별\", \"무시당함\": \"무시\",\n",
        "    \"무시함\": \"무시\", \"편견들\": \"편견\", \"회사\": \"직장\", \"업무\": \"직장\",\n",
        "    \"일\": \"직장\", \"회사생활\": \"직장\", \"말\": \"언어\", \"한국사람\": \"한국인\"\n",
        "}\n",
        "def clean_text(text):\n",
        "    words = text.split()\n",
        "    processed = []\n",
        "    for word in words:\n",
        "        word = standardization_dict.get(word, word)\n",
        "        if word not in stopwords:\n",
        "            processed.append(word)\n",
        "    return \" \".join(processed)"
      ],
      "metadata": {
        "id": "K4o7baa8bTsK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs_cleaned = [\n",
        "    clean_text(text)\n",
        "    for text in df[\"원자료\"].dropna().astype(str).tolist() ]"
      ],
      "metadata": {
        "id": "vs8aF1UebV3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text, stopwords, standardization_dict):\n",
        "    words = text.split()\n",
        "    processed = []\n",
        "    for word in words:\n",
        "        word = standardization_dict.get(word, word)\n",
        "        if word not in stopwords:\n",
        "            processed.append(word)\n",
        "    return processed\n",
        "\n",
        "docs_tokenized = [preprocess_text(doc, stopwords, standardization_dict)\n",
        "    for doc in docs_cleaned]\n",
        "\n",
        "from gensim.corpora import Dictionary\n",
        "id2word = Dictionary(docs_tokenized)"
      ],
      "metadata": {
        "id": "rXMqRMDUbYfj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_nums = list(range(3, 21))\n",
        "\n",
        "results = []\n",
        "\n",
        "embedding_model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
        "\n",
        "for n in tqdm(topic_nums):\n",
        "    topic_model = BERTopic(\n",
        "        embedding_model=embedding_model,\n",
        "        language=\"multilingual\",\n",
        "        calculate_probabilities=False,\n",
        "        low_memory=True,\n",
        "        n_gram_range=(1, 3)\n",
        "    )\n",
        "\n",
        "    topics, probs = topic_model.fit_transform(docs_cleaned)\n",
        "\n",
        "    reduced_model = topic_model.reduce_topics(docs_cleaned, nr_topics=n)\n",
        "\n",
        "    keywords = [reduced_model.get_topic(i) for i in range(n)]\n",
        "    keywords = [topic for topic in keywords if topic]\n",
        "    topic_words = [[word for word, _ in topic] for topic in keywords]\n",
        "\n",
        "    topic_reprs = reduced_model.topic_embeddings_\n",
        "    sim_matrix = cosine_similarity(topic_reprs)\n",
        "    upper_tri = sim_matrix[np.triu_indices_from(sim_matrix, k=1)]\n",
        "    similarity = np.mean(upper_tri)\n",
        "    cosine_distance = 1 - similarity\n",
        "\n",
        "    cm = CoherenceModel(\n",
        "        topics=topic_words,\n",
        "        texts=docs_tokenized,\n",
        "        dictionary=id2word,\n",
        "        coherence='c_v',\n",
        "        processes=1)\n",
        "    coherence = cm.get_coherence()\n",
        "\n",
        "    results.append({\n",
        "        \"토픽수\": n,\n",
        "        \"코사인거리(Cosine Distance)\": cosine_distance,\n",
        "        \"일관성(Coherence)\": coherence\n",
        "    })\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df"
      ],
      "metadata": {
        "id": "P2xWZNnObZur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "output_path = \"/content/토픽수_코사인거리_코히런스결과.xlsx\"\n",
        "results_df.to_excel(output_path, index=False)\n",
        "files.download(output_path)"
      ],
      "metadata": {
        "id": "RLLH6NY8bbPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q sentence-transformers scikit-learn openpyxl\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files\n",
        "import io\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for filename in uploaded:\n",
        "    if filename.endswith(\".xlsx\"):\n",
        "        df = pd.read_excel(io.BytesIO(uploaded[filename]))\n",
        "    elif filename.endswith(\".csv\"):\n",
        "        df = pd.read_csv(io.BytesIO(uploaded[filename]))\n",
        "    else:\n",
        "        raise ValueError(\"지원되지 않는 파일 형식입니다.\")\n",
        "\n",
        "stopwords = set([\n",
        "    '그', '이', '저', '것', '등', '때', '중', '누구', '무엇', '하다', '되다', '있다', '없다', '받다',\n",
        "    '가다', '오다', '보다', '주다', '말하다', '들다', '계속', '이제', '지금', '예전', '요즘', '거의',\n",
        "    '좀', '많이', '더', '다시', '별로', '그리고', '그래서', '그런데', '하지만', '그러니까', '때문',\n",
        "    '이런', '그런', '어떤', '같다', '나', '내', '너', '우리', '당신', '그녀', '그들', '사람', '모두',\n",
        "    '여기', '저기', '거기', '안', '밖', '위', '아래', '그곳', '방향', '도', '는', '만', '과', '와',\n",
        "    '보다', '까지', '부터', '으로', '에게', '이랑', '밖에', '조차', '정도', '경우', '내용', '문제',\n",
        "    '이야기', '모습', '상황', '자신', '음', '응', '오', '어휴', '허', '아이고', '에휴', '웃음',\n",
        "    '그떄', '그다음', '아마', '...', '…', '“', '”', '‘', '’', '-', '--', '―', '그냥', '진짜', '완전',\n",
        "    '약간', '어떻게', '뭐', '딱', '막', '또', '또는', '해도', '생각', '중략', '가지', '조금',\n",
        "    '다문화', '중국', '일본', '한국', '우즈베키스탄', '베트남', '태국', '몽골', '몽고',\n",
        "    '자기', '그때', '그거', '수도', '그게', '여러', '무슨', '네네', '나이', '어디', '먼저', '대부분',\n",
        "    '나중', '대해', '그것', '뭔가', '전혀', '저희', '만약', '이주', '나가야', '다른', '항상',\n",
        "    '얘기', '나라', '부분', '선생님'\n",
        "])\n",
        "\n",
        "standardization_dict = {\n",
        "    \"아빠\": \"남편\", \"여자\": \"여성\", \"아기\": \"아이\", \"한국말\": \"한국어\",\n",
        "    \"시엄마\": \"시어머니\", \"어머니\": \"시어머니\", \"대한민국\": \"한국\",\n",
        "    \"외국\": \"외국인\", \"외국사람\": \"외국인\", \"차별적\": \"차별\", \"무시당함\": \"무시\",\n",
        "    \"무시함\": \"무시\", \"편견들\": \"편견\", \"회사\": \"직장\", \"업무\": \"직장\",\n",
        "    \"일\": \"직장\", \"회사생활\": \"직장\", \"말\": \"언어\", \"한국사람\": \"한국인\"\n",
        "}\n",
        "\n",
        "def preprocess_text(text):\n",
        "    words = text.split()\n",
        "    cleaned = []\n",
        "    for word in words:\n",
        "        word = standardization_dict.get(word, word)\n",
        "        if word not in stopwords:\n",
        "            cleaned.append(word)\n",
        "    return \" \".join(cleaned)\n",
        "\n",
        "sentences = [preprocess_text(doc) for doc in df[\"원자료\"].dropna().astype(str).tolist()]\n",
        "\n",
        "model = SentenceTransformer(\"distiluse-base-multilingual-cased-v2\")\n",
        "embeddings = model.encode(sentences)\n",
        "\n",
        "num_topics = 7\n",
        "kmeans = KMeans(n_clusters=num_topics, random_state=42)\n",
        "labels = kmeans.fit_predict(embeddings)\n",
        "\n",
        "df = df.loc[df[\"원자료\"].notna()].copy()\n",
        "df[\"토픽번호\"] = labels\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "reduced = pca.fit_transform(embeddings)\n",
        "df[\"x\"] = reduced[:, 0]\n",
        "df[\"y\"] = reduced[:, 1]\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "for label in sorted(set(labels)):\n",
        "    subset = df[df[\"토픽번호\"] == label]\n",
        "    plt.scatter(subset[\"x\"], subset[\"y\"], label=f\"Topic {label}\")\n",
        "plt.title(\"BERT 임베딩 + KMeans 클러스터링 (토픽 수: 7)\")\n",
        "plt.xlabel(\"PCA 1\")\n",
        "plt.ylabel(\"PCA 2\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "output_path = \"/content/최종_토픽_분석결과_7개.xlsx\"\n",
        "df.to_excel(output_path, index=False)\n",
        "files.download(output_path)"
      ],
      "metadata": {
        "id": "FpV5zf0Ubdxw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "import matplotlib.font_manager as fm\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "\n",
        "font_path = \"/content/NanumBarunGothic.ttf\"\n",
        "fm.fontManager.addfont(font_path)\n",
        "font_name = fm.FontProperties(fname=font_path).get_name()\n",
        "\n",
        "mpl.rc('font', family=font_name)\n",
        "plt.rcParams['axes.unicode_minus'] = False"
      ],
      "metadata": {
        "id": "1Yu8jRLEbfIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "topic_labels = {\n",
        "    0: \"외모/출신 국가에 대한 편견\",\n",
        "    1: \"교육장면에서 발생하는 차별\",\n",
        "    2: \"언어에 대한 차별\",\n",
        "    3: \"무시와 인격적 경시\",\n",
        "    4: \"사회적 배제와 소외시킴\",\n",
        "    5: \"제도적 낙인과 의심\",\n",
        "    6: \"성역할 고정관념\"\n",
        "}\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "for topic_num in df['토픽번호'].unique():\n",
        "    subset = df[df['토픽번호'] == topic_num]\n",
        "    plt.scatter(subset['x'], subset['y'], label=topic_labels[topic_num], alpha=0.7)\n",
        "\n",
        "plt.title(\"BERT 임베딩 + KMeans 클러스터링 (사용자 정의 토픽명)\", fontsize=14)\n",
        "plt.xlabel(\"PCA 1\")\n",
        "plt.ylabel(\"PCA 2\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "S-PPnLrxbiUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "original_sentences = df[\"원자료\"].tolist()\n",
        "\n",
        "for topic_idx in range(7):\n",
        "    print(f\"\\n🧩 토픽 {topic_idx} 대표 문장:\")\n",
        "\n",
        "    center_vector = kmeans.cluster_centers_[topic_idx].reshape(1, -1)\n",
        "    sims = cosine_similarity(center_vector, embeddings)[0]\n",
        "\n",
        "    topic_sentences_idx = np.where(labels == topic_idx)[0]\n",
        "    top_idxs = topic_sentences_idx[np.argsort(sims[topic_sentences_idx])[::-1][:5]]\n",
        "\n",
        "    for idx in top_idxs:\n",
        "        print(f\"- {original_sentences[idx]}\")"
      ],
      "metadata": {
        "id": "rrZlylYsblCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install konlpy\n",
        "from konlpy.tag import Okt\n",
        "okt = Okt()"
      ],
      "metadata": {
        "id": "zCrJl8X5NhbI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topwords = set([\n",
        "    '그', '이', '저', '것', '등', '때', '중', '누구', '무엇',\n",
        "    '하다', '되다', '있다', '없다', '받다', '가다', '오다', '보다', '주다', '말하다', '들다',\n",
        "    '계속', '이제', '지금', '예전', '요즘', '거의', '좀', '많이', '더', '다시', '별로',\n",
        "    '그리고', '그래서', '그런데', '하지만', '그러니까', '때문', '이런', '그런', '어떤', '같다',\n",
        "    '나', '내', '너', '우리', '당신', '그녀', '그들', '사람', '모두',\n",
        "    '여기', '저기', '거기', '안', '밖', '위', '아래', '그곳', '방향',\n",
        "    '도', '는', '만', '과', '와', '보다', '까지', '부터', '으로', '에게', '이랑', '밖에', '조차',\n",
        "    '정도', '경우', '내용', '문제', '이야기', '모습', '상황', '자신',\n",
        "    '음', '응', '오', '어휴', '허', '아이고', '에휴', '웃음', '그떄', '그다음', '아마',\n",
        "    '...', '…', '“', '”', '‘', '’', '-', '--', '―',\n",
        "    '그냥', '진짜', '완전', '약간', '어떻게', '뭐', '딱', '막', '또', '또는',\n",
        "    '해도', '생각', '중략', '가지', '조금', '다문화', '이제',\n",
        "    '중국', '일본', '한국', '우즈베키스탄', '베트남', '태국', '몽골', '몽고',\n",
        "    '자기', '그때', '그거', '수도', '그게', '여러', '무슨', '네네', '나이', '어디',\n",
        "    '먼저', '대부분', '나중', '대해', '그것', '뭔가', '전혀', '저희', '만약', '이주', '나가야',\n",
        "   '다른', '항상',  '얘기', '나라', '부분', '선생님'\n",
        "])\n",
        "standardization_dict = {\n",
        "      \"아빠\": \"남편\", \"여자\":\"여성\", \"아기\":\"아이\",\n",
        "    \"한국말\": \"한국어\", \"시엄마\": \"시어머니\", \"어머니\": \"시어머니\", \"대한민국\": \"한국\",\n",
        "    \"외국\": \"외국인\", \"외국사람\": \"외국인\",\n",
        "    \"차별적\": \"차별\",\n",
        "    \"무시당함\": \"무시\", \"무시함\": \"무시\",\n",
        "    \"편견들\": \"편견\", \"회사\":\"직장\", \"업무\":\"직장\", \"일\":\"직장\",\"회사생활\":\"직장\",\n",
        "    \"말\": \"언어\",\n",
        "    \"한국사람\": \"한국인\"\n",
        "}"
      ],
      "metadata": {
        "id": "Q248-72nNkM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_keywords(text):\n",
        "    words = text.split()\n",
        "    standardized_words = [standardization_dict.get(w, w) for w in words]\n",
        "\n",
        "    keywords = []\n",
        "    for word in standardized_words:\n",
        "        nouns = okt.nouns(word)\n",
        "        for noun in nouns:\n",
        "            if noun not in stopwords and len(noun) > 1:\n",
        "                keywords.append(noun)\n",
        "    return keywords"
      ],
      "metadata": {
        "id": "dlWyLejpNmTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "print(\"\\n📌 각 토픽별 상위 키워드 (형태소 + 불용어 제거 + 표준화):\")\n",
        "\n",
        "for topic_idx in range(7):\n",
        "    topic_docs = [\n",
        "        extract_keywords(text)\n",
        "        for i, text in enumerate(original_sentences)\n",
        "        if labels[i] == topic_idx\n",
        "    ]\n",
        "    all_words = [word for doc in topic_docs for word in doc]\n",
        "\n",
        "    if not all_words:\n",
        "        print(f\"\\n🧩 토픽 {topic_idx}: 키워드 없음\")\n",
        "        continue\n",
        "\n",
        "    top_words = Counter(all_words).most_common(10)\n",
        "    print(f\"\\n🧩 토픽 {topic_idx} 키워드:\")\n",
        "    for word, freq in top_words:\n",
        "        print(f\"- {word} ({freq})\")"
      ],
      "metadata": {
        "id": "KnU2YcCqNoUm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}