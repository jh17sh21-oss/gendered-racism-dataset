{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --force-reinstall numpy\n",
        "!pip install bertopic gensim scikit-learn sentence-transformers tqdm openpyxl"
      ],
      "metadata": {
        "id": "d4SnBjl1cKjw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from bertopic import BERTopic\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from gensim.models import CoherenceModel\n",
        "from gensim.corpora import Dictionary\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from google.colab import files"
      ],
      "metadata": {
        "id": "SV6TyZwEbQAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "import io\n",
        "import pandas as pd\n",
        "\n",
        "for filename in uploaded:\n",
        "    if filename.endswith(\".xlsx\"):\n",
        "        df = pd.read_excel(io.BytesIO(uploaded[filename]))\n",
        "    elif filename.endswith(\".csv\"):\n",
        "        df = pd.read_csv(io.BytesIO(uploaded[filename]))\n",
        "    else:\n",
        "        raise ValueError(\"ì§€ì›ë˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤. .xlsx ë˜ëŠ” .csvë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.\")"
      ],
      "metadata": {
        "id": "g8VZE5MBbR6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords = set([\n",
        "    'ê·¸', 'ì´', 'ì €', 'ê²ƒ', 'ë“±', 'ë•Œ', 'ì¤‘', 'ëˆ„êµ¬', 'ë¬´ì—‡', 'í•˜ë‹¤', 'ë˜ë‹¤', 'ìˆë‹¤', 'ì—†ë‹¤', 'ë°›ë‹¤',\n",
        "    'ê°€ë‹¤', 'ì˜¤ë‹¤', 'ë³´ë‹¤', 'ì£¼ë‹¤', 'ë§í•˜ë‹¤', 'ë“¤ë‹¤', 'ê³„ì†', 'ì´ì œ', 'ì§€ê¸ˆ', 'ì˜ˆì „', 'ìš”ì¦˜', 'ê±°ì˜',\n",
        "    'ì¢€', 'ë§ì´', 'ë”', 'ë‹¤ì‹œ', 'ë³„ë¡œ', 'ê·¸ë¦¬ê³ ', 'ê·¸ë˜ì„œ', 'ê·¸ëŸ°ë°', 'í•˜ì§€ë§Œ', 'ê·¸ëŸ¬ë‹ˆê¹Œ', 'ë•Œë¬¸',\n",
        "    'ì´ëŸ°', 'ê·¸ëŸ°', 'ì–´ë–¤', 'ê°™ë‹¤', 'ë‚˜', 'ë‚´', 'ë„ˆ', 'ìš°ë¦¬', 'ë‹¹ì‹ ', 'ê·¸ë…€', 'ê·¸ë“¤', 'ì‚¬ëŒ', 'ëª¨ë‘',\n",
        "    'ì—¬ê¸°', 'ì €ê¸°', 'ê±°ê¸°', 'ì•ˆ', 'ë°–', 'ìœ„', 'ì•„ë˜', 'ê·¸ê³³', 'ë°©í–¥', 'ë„', 'ëŠ”', 'ë§Œ', 'ê³¼', 'ì™€',\n",
        "    'ë³´ë‹¤', 'ê¹Œì§€', 'ë¶€í„°', 'ìœ¼ë¡œ', 'ì—ê²Œ', 'ì´ë‘', 'ë°–ì—', 'ì¡°ì°¨', 'ì •ë„', 'ê²½ìš°', 'ë‚´ìš©', 'ë¬¸ì œ',\n",
        "    'ì´ì•¼ê¸°', 'ëª¨ìŠµ', 'ìƒí™©', 'ìì‹ ', 'ìŒ', 'ì‘', 'ì˜¤', 'ì–´íœ´', 'í—ˆ', 'ì•„ì´ê³ ', 'ì—íœ´', 'ì›ƒìŒ',\n",
        "    'ê·¸ë–„', 'ê·¸ë‹¤ìŒ', 'ì•„ë§ˆ', '...', 'â€¦', 'â€œ', 'â€', 'â€˜', 'â€™', '-', '--', 'â€•', 'ê·¸ëƒ¥', 'ì§„ì§œ', 'ì™„ì „',\n",
        "    'ì•½ê°„', 'ì–´ë–»ê²Œ', 'ë­', 'ë”±', 'ë§‰', 'ë˜', 'ë˜ëŠ”', 'í•´ë„', 'ìƒê°', 'ì¤‘ëµ', 'ê°€ì§€', 'ì¡°ê¸ˆ',\n",
        "    'ë‹¤ë¬¸í™”', 'ì¤‘êµ­', 'ì¼ë³¸', 'í•œêµ­', 'ìš°ì¦ˆë² í‚¤ìŠ¤íƒ„', 'ë² íŠ¸ë‚¨', 'íƒœêµ­', 'ëª½ê³¨', 'ëª½ê³ ',\n",
        "    'ìê¸°', 'ê·¸ë•Œ', 'ê·¸ê±°', 'ìˆ˜ë„', 'ê·¸ê²Œ', 'ì—¬ëŸ¬', 'ë¬´ìŠ¨', 'ë„¤ë„¤', 'ë‚˜ì´', 'ì–´ë””', 'ë¨¼ì €', 'ëŒ€ë¶€ë¶„',\n",
        "    'ë‚˜ì¤‘', 'ëŒ€í•´', 'ê·¸ê²ƒ', 'ë­”ê°€', 'ì „í˜€', 'ì €í¬', 'ë§Œì•½', 'ì´ì£¼', 'ë‚˜ê°€ì•¼', 'ë‹¤ë¥¸', 'í•­ìƒ',\n",
        "    'ì–˜ê¸°',\n",
        "])\n",
        "\n",
        "standardization_dict = {\n",
        "    \"ì•„ë¹ \": \"ë‚¨í¸\", \"ì—¬ì\": \"ì—¬ì„±\", \"ì•„ê¸°\": \"ì•„ì´\", \"í•œêµ­ë§\": \"í•œêµ­ì–´\",\n",
        "    \"ì‹œì—„ë§ˆ\": \"ì‹œì–´ë¨¸ë‹ˆ\", \"ì–´ë¨¸ë‹ˆ\": \"ì‹œì–´ë¨¸ë‹ˆ\", \"ëŒ€í•œë¯¼êµ­\": \"í•œêµ­\",\n",
        "    \"ì™¸êµ­\": \"ì™¸êµ­ì¸\", \"ì™¸êµ­ì‚¬ëŒ\": \"ì™¸êµ­ì¸\", \"ì°¨ë³„ì \": \"ì°¨ë³„\", \"ë¬´ì‹œë‹¹í•¨\": \"ë¬´ì‹œ\",\n",
        "    \"ë¬´ì‹œí•¨\": \"ë¬´ì‹œ\", \"í¸ê²¬ë“¤\": \"í¸ê²¬\", \"íšŒì‚¬\": \"ì§ì¥\", \"ì—…ë¬´\": \"ì§ì¥\",\n",
        "    \"ì¼\": \"ì§ì¥\", \"íšŒì‚¬ìƒí™œ\": \"ì§ì¥\", \"ë§\": \"ì–¸ì–´\", \"í•œêµ­ì‚¬ëŒ\": \"í•œêµ­ì¸\"\n",
        "}\n",
        "def clean_text(text):\n",
        "    words = text.split()\n",
        "    processed = []\n",
        "    for word in words:\n",
        "        word = standardization_dict.get(word, word)\n",
        "        if word not in stopwords:\n",
        "            processed.append(word)\n",
        "    return \" \".join(processed)"
      ],
      "metadata": {
        "id": "K4o7baa8bTsK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs_cleaned = [\n",
        "    clean_text(text)\n",
        "    for text in df[\"ì›ìë£Œ\"].dropna().astype(str).tolist() ]"
      ],
      "metadata": {
        "id": "vs8aF1UebV3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text, stopwords, standardization_dict):\n",
        "    words = text.split()\n",
        "    processed = []\n",
        "    for word in words:\n",
        "        word = standardization_dict.get(word, word)\n",
        "        if word not in stopwords:\n",
        "            processed.append(word)\n",
        "    return processed\n",
        "\n",
        "docs_tokenized = [preprocess_text(doc, stopwords, standardization_dict)\n",
        "    for doc in docs_cleaned]\n",
        "\n",
        "from gensim.corpora import Dictionary\n",
        "id2word = Dictionary(docs_tokenized)"
      ],
      "metadata": {
        "id": "rXMqRMDUbYfj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_nums = list(range(3, 21))\n",
        "\n",
        "results = []\n",
        "\n",
        "embedding_model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
        "\n",
        "for n in tqdm(topic_nums):\n",
        "    topic_model = BERTopic(\n",
        "        embedding_model=embedding_model,\n",
        "        language=\"multilingual\",\n",
        "        calculate_probabilities=False,\n",
        "        low_memory=True,\n",
        "        n_gram_range=(1, 3)\n",
        "    )\n",
        "\n",
        "    topics, probs = topic_model.fit_transform(docs_cleaned)\n",
        "\n",
        "    reduced_model = topic_model.reduce_topics(docs_cleaned, nr_topics=n)\n",
        "\n",
        "    keywords = [reduced_model.get_topic(i) for i in range(n)]\n",
        "    keywords = [topic for topic in keywords if topic]\n",
        "    topic_words = [[word for word, _ in topic] for topic in keywords]\n",
        "\n",
        "    topic_reprs = reduced_model.topic_embeddings_\n",
        "    sim_matrix = cosine_similarity(topic_reprs)\n",
        "    upper_tri = sim_matrix[np.triu_indices_from(sim_matrix, k=1)]\n",
        "    similarity = np.mean(upper_tri)\n",
        "    cosine_distance = 1 - similarity\n",
        "\n",
        "    cm = CoherenceModel(\n",
        "        topics=topic_words,\n",
        "        texts=docs_tokenized,\n",
        "        dictionary=id2word,\n",
        "        coherence='c_v',\n",
        "        processes=1)\n",
        "    coherence = cm.get_coherence()\n",
        "\n",
        "    results.append({\n",
        "        \"í† í”½ìˆ˜\": n,\n",
        "        \"ì½”ì‚¬ì¸ê±°ë¦¬(Cosine Distance)\": cosine_distance,\n",
        "        \"ì¼ê´€ì„±(Coherence)\": coherence\n",
        "    })\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df"
      ],
      "metadata": {
        "id": "P2xWZNnObZur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "output_path = \"/content/í† í”½ìˆ˜_ì½”ì‚¬ì¸ê±°ë¦¬_ì½”íˆëŸ°ìŠ¤ê²°ê³¼.xlsx\"\n",
        "results_df.to_excel(output_path, index=False)\n",
        "files.download(output_path)"
      ],
      "metadata": {
        "id": "RLLH6NY8bbPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q sentence-transformers scikit-learn openpyxl\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files\n",
        "import io\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for filename in uploaded:\n",
        "    if filename.endswith(\".xlsx\"):\n",
        "        df = pd.read_excel(io.BytesIO(uploaded[filename]))\n",
        "    elif filename.endswith(\".csv\"):\n",
        "        df = pd.read_csv(io.BytesIO(uploaded[filename]))\n",
        "    else:\n",
        "        raise ValueError(\"ì§€ì›ë˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤.\")\n",
        "\n",
        "stopwords = set([\n",
        "    'ê·¸', 'ì´', 'ì €', 'ê²ƒ', 'ë“±', 'ë•Œ', 'ì¤‘', 'ëˆ„êµ¬', 'ë¬´ì—‡', 'í•˜ë‹¤', 'ë˜ë‹¤', 'ìˆë‹¤', 'ì—†ë‹¤', 'ë°›ë‹¤',\n",
        "    'ê°€ë‹¤', 'ì˜¤ë‹¤', 'ë³´ë‹¤', 'ì£¼ë‹¤', 'ë§í•˜ë‹¤', 'ë“¤ë‹¤', 'ê³„ì†', 'ì´ì œ', 'ì§€ê¸ˆ', 'ì˜ˆì „', 'ìš”ì¦˜', 'ê±°ì˜',\n",
        "    'ì¢€', 'ë§ì´', 'ë”', 'ë‹¤ì‹œ', 'ë³„ë¡œ', 'ê·¸ë¦¬ê³ ', 'ê·¸ë˜ì„œ', 'ê·¸ëŸ°ë°', 'í•˜ì§€ë§Œ', 'ê·¸ëŸ¬ë‹ˆê¹Œ', 'ë•Œë¬¸',\n",
        "    'ì´ëŸ°', 'ê·¸ëŸ°', 'ì–´ë–¤', 'ê°™ë‹¤', 'ë‚˜', 'ë‚´', 'ë„ˆ', 'ìš°ë¦¬', 'ë‹¹ì‹ ', 'ê·¸ë…€', 'ê·¸ë“¤', 'ì‚¬ëŒ', 'ëª¨ë‘',\n",
        "    'ì—¬ê¸°', 'ì €ê¸°', 'ê±°ê¸°', 'ì•ˆ', 'ë°–', 'ìœ„', 'ì•„ë˜', 'ê·¸ê³³', 'ë°©í–¥', 'ë„', 'ëŠ”', 'ë§Œ', 'ê³¼', 'ì™€',\n",
        "    'ë³´ë‹¤', 'ê¹Œì§€', 'ë¶€í„°', 'ìœ¼ë¡œ', 'ì—ê²Œ', 'ì´ë‘', 'ë°–ì—', 'ì¡°ì°¨', 'ì •ë„', 'ê²½ìš°', 'ë‚´ìš©', 'ë¬¸ì œ',\n",
        "    'ì´ì•¼ê¸°', 'ëª¨ìŠµ', 'ìƒí™©', 'ìì‹ ', 'ìŒ', 'ì‘', 'ì˜¤', 'ì–´íœ´', 'í—ˆ', 'ì•„ì´ê³ ', 'ì—íœ´', 'ì›ƒìŒ',\n",
        "    'ê·¸ë–„', 'ê·¸ë‹¤ìŒ', 'ì•„ë§ˆ', '...', 'â€¦', 'â€œ', 'â€', 'â€˜', 'â€™', '-', '--', 'â€•', 'ê·¸ëƒ¥', 'ì§„ì§œ', 'ì™„ì „',\n",
        "    'ì•½ê°„', 'ì–´ë–»ê²Œ', 'ë­', 'ë”±', 'ë§‰', 'ë˜', 'ë˜ëŠ”', 'í•´ë„', 'ìƒê°', 'ì¤‘ëµ', 'ê°€ì§€', 'ì¡°ê¸ˆ',\n",
        "    'ë‹¤ë¬¸í™”', 'ì¤‘êµ­', 'ì¼ë³¸', 'í•œêµ­', 'ìš°ì¦ˆë² í‚¤ìŠ¤íƒ„', 'ë² íŠ¸ë‚¨', 'íƒœêµ­', 'ëª½ê³¨', 'ëª½ê³ ',\n",
        "    'ìê¸°', 'ê·¸ë•Œ', 'ê·¸ê±°', 'ìˆ˜ë„', 'ê·¸ê²Œ', 'ì—¬ëŸ¬', 'ë¬´ìŠ¨', 'ë„¤ë„¤', 'ë‚˜ì´', 'ì–´ë””', 'ë¨¼ì €', 'ëŒ€ë¶€ë¶„',\n",
        "    'ë‚˜ì¤‘', 'ëŒ€í•´', 'ê·¸ê²ƒ', 'ë­”ê°€', 'ì „í˜€', 'ì €í¬', 'ë§Œì•½', 'ì´ì£¼', 'ë‚˜ê°€ì•¼', 'ë‹¤ë¥¸', 'í•­ìƒ',\n",
        "    'ì–˜ê¸°', 'ë‚˜ë¼', 'ë¶€ë¶„', 'ì„ ìƒë‹˜'\n",
        "])\n",
        "\n",
        "standardization_dict = {\n",
        "    \"ì•„ë¹ \": \"ë‚¨í¸\", \"ì—¬ì\": \"ì—¬ì„±\", \"ì•„ê¸°\": \"ì•„ì´\", \"í•œêµ­ë§\": \"í•œêµ­ì–´\",\n",
        "    \"ì‹œì—„ë§ˆ\": \"ì‹œì–´ë¨¸ë‹ˆ\", \"ì–´ë¨¸ë‹ˆ\": \"ì‹œì–´ë¨¸ë‹ˆ\", \"ëŒ€í•œë¯¼êµ­\": \"í•œêµ­\",\n",
        "    \"ì™¸êµ­\": \"ì™¸êµ­ì¸\", \"ì™¸êµ­ì‚¬ëŒ\": \"ì™¸êµ­ì¸\", \"ì°¨ë³„ì \": \"ì°¨ë³„\", \"ë¬´ì‹œë‹¹í•¨\": \"ë¬´ì‹œ\",\n",
        "    \"ë¬´ì‹œí•¨\": \"ë¬´ì‹œ\", \"í¸ê²¬ë“¤\": \"í¸ê²¬\", \"íšŒì‚¬\": \"ì§ì¥\", \"ì—…ë¬´\": \"ì§ì¥\",\n",
        "    \"ì¼\": \"ì§ì¥\", \"íšŒì‚¬ìƒí™œ\": \"ì§ì¥\", \"ë§\": \"ì–¸ì–´\", \"í•œêµ­ì‚¬ëŒ\": \"í•œêµ­ì¸\"\n",
        "}\n",
        "\n",
        "def preprocess_text(text):\n",
        "    words = text.split()\n",
        "    cleaned = []\n",
        "    for word in words:\n",
        "        word = standardization_dict.get(word, word)\n",
        "        if word not in stopwords:\n",
        "            cleaned.append(word)\n",
        "    return \" \".join(cleaned)\n",
        "\n",
        "sentences = [preprocess_text(doc) for doc in df[\"ì›ìë£Œ\"].dropna().astype(str).tolist()]\n",
        "\n",
        "model = SentenceTransformer(\"distiluse-base-multilingual-cased-v2\")\n",
        "embeddings = model.encode(sentences)\n",
        "\n",
        "num_topics = 7\n",
        "kmeans = KMeans(n_clusters=num_topics, random_state=42)\n",
        "labels = kmeans.fit_predict(embeddings)\n",
        "\n",
        "df = df.loc[df[\"ì›ìë£Œ\"].notna()].copy()\n",
        "df[\"í† í”½ë²ˆí˜¸\"] = labels\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "reduced = pca.fit_transform(embeddings)\n",
        "df[\"x\"] = reduced[:, 0]\n",
        "df[\"y\"] = reduced[:, 1]\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "for label in sorted(set(labels)):\n",
        "    subset = df[df[\"í† í”½ë²ˆí˜¸\"] == label]\n",
        "    plt.scatter(subset[\"x\"], subset[\"y\"], label=f\"Topic {label}\")\n",
        "plt.title(\"BERT ì„ë² ë”© + KMeans í´ëŸ¬ìŠ¤í„°ë§ (í† í”½ ìˆ˜: 7)\")\n",
        "plt.xlabel(\"PCA 1\")\n",
        "plt.ylabel(\"PCA 2\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "output_path = \"/content/ìµœì¢…_í† í”½_ë¶„ì„ê²°ê³¼_7ê°œ.xlsx\"\n",
        "df.to_excel(output_path, index=False)\n",
        "files.download(output_path)"
      ],
      "metadata": {
        "id": "FpV5zf0Ubdxw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "import matplotlib.font_manager as fm\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "\n",
        "font_path = \"/content/NanumBarunGothic.ttf\"\n",
        "fm.fontManager.addfont(font_path)\n",
        "font_name = fm.FontProperties(fname=font_path).get_name()\n",
        "\n",
        "mpl.rc('font', family=font_name)\n",
        "plt.rcParams['axes.unicode_minus'] = False"
      ],
      "metadata": {
        "id": "1Yu8jRLEbfIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "topic_labels = {\n",
        "    0: \"ì™¸ëª¨/ì¶œì‹  êµ­ê°€ì— ëŒ€í•œ í¸ê²¬\",\n",
        "    1: \"êµìœ¡ì¥ë©´ì—ì„œ ë°œìƒí•˜ëŠ” ì°¨ë³„\",\n",
        "    2: \"ì–¸ì–´ì— ëŒ€í•œ ì°¨ë³„\",\n",
        "    3: \"ë¬´ì‹œì™€ ì¸ê²©ì  ê²½ì‹œ\",\n",
        "    4: \"ì‚¬íšŒì  ë°°ì œì™€ ì†Œì™¸ì‹œí‚´\",\n",
        "    5: \"ì œë„ì  ë‚™ì¸ê³¼ ì˜ì‹¬\",\n",
        "    6: \"ì„±ì—­í•  ê³ ì •ê´€ë…\"\n",
        "}\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "for topic_num in df['í† í”½ë²ˆí˜¸'].unique():\n",
        "    subset = df[df['í† í”½ë²ˆí˜¸'] == topic_num]\n",
        "    plt.scatter(subset['x'], subset['y'], label=topic_labels[topic_num], alpha=0.7)\n",
        "\n",
        "plt.title(\"BERT ì„ë² ë”© + KMeans í´ëŸ¬ìŠ¤í„°ë§ (ì‚¬ìš©ì ì •ì˜ í† í”½ëª…)\", fontsize=14)\n",
        "plt.xlabel(\"PCA 1\")\n",
        "plt.ylabel(\"PCA 2\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "S-PPnLrxbiUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "original_sentences = df[\"ì›ìë£Œ\"].tolist()\n",
        "\n",
        "for topic_idx in range(7):\n",
        "    print(f\"\\nğŸ§© í† í”½ {topic_idx} ëŒ€í‘œ ë¬¸ì¥:\")\n",
        "\n",
        "    center_vector = kmeans.cluster_centers_[topic_idx].reshape(1, -1)\n",
        "    sims = cosine_similarity(center_vector, embeddings)[0]\n",
        "\n",
        "    topic_sentences_idx = np.where(labels == topic_idx)[0]\n",
        "    top_idxs = topic_sentences_idx[np.argsort(sims[topic_sentences_idx])[::-1][:5]]\n",
        "\n",
        "    for idx in top_idxs:\n",
        "        print(f\"- {original_sentences[idx]}\")"
      ],
      "metadata": {
        "id": "rrZlylYsblCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install konlpy\n",
        "from konlpy.tag import Okt\n",
        "okt = Okt()"
      ],
      "metadata": {
        "id": "zCrJl8X5NhbI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topwords = set([\n",
        "    'ê·¸', 'ì´', 'ì €', 'ê²ƒ', 'ë“±', 'ë•Œ', 'ì¤‘', 'ëˆ„êµ¬', 'ë¬´ì—‡',\n",
        "    'í•˜ë‹¤', 'ë˜ë‹¤', 'ìˆë‹¤', 'ì—†ë‹¤', 'ë°›ë‹¤', 'ê°€ë‹¤', 'ì˜¤ë‹¤', 'ë³´ë‹¤', 'ì£¼ë‹¤', 'ë§í•˜ë‹¤', 'ë“¤ë‹¤',\n",
        "    'ê³„ì†', 'ì´ì œ', 'ì§€ê¸ˆ', 'ì˜ˆì „', 'ìš”ì¦˜', 'ê±°ì˜', 'ì¢€', 'ë§ì´', 'ë”', 'ë‹¤ì‹œ', 'ë³„ë¡œ',\n",
        "    'ê·¸ë¦¬ê³ ', 'ê·¸ë˜ì„œ', 'ê·¸ëŸ°ë°', 'í•˜ì§€ë§Œ', 'ê·¸ëŸ¬ë‹ˆê¹Œ', 'ë•Œë¬¸', 'ì´ëŸ°', 'ê·¸ëŸ°', 'ì–´ë–¤', 'ê°™ë‹¤',\n",
        "    'ë‚˜', 'ë‚´', 'ë„ˆ', 'ìš°ë¦¬', 'ë‹¹ì‹ ', 'ê·¸ë…€', 'ê·¸ë“¤', 'ì‚¬ëŒ', 'ëª¨ë‘',\n",
        "    'ì—¬ê¸°', 'ì €ê¸°', 'ê±°ê¸°', 'ì•ˆ', 'ë°–', 'ìœ„', 'ì•„ë˜', 'ê·¸ê³³', 'ë°©í–¥',\n",
        "    'ë„', 'ëŠ”', 'ë§Œ', 'ê³¼', 'ì™€', 'ë³´ë‹¤', 'ê¹Œì§€', 'ë¶€í„°', 'ìœ¼ë¡œ', 'ì—ê²Œ', 'ì´ë‘', 'ë°–ì—', 'ì¡°ì°¨',\n",
        "    'ì •ë„', 'ê²½ìš°', 'ë‚´ìš©', 'ë¬¸ì œ', 'ì´ì•¼ê¸°', 'ëª¨ìŠµ', 'ìƒí™©', 'ìì‹ ',\n",
        "    'ìŒ', 'ì‘', 'ì˜¤', 'ì–´íœ´', 'í—ˆ', 'ì•„ì´ê³ ', 'ì—íœ´', 'ì›ƒìŒ', 'ê·¸ë–„', 'ê·¸ë‹¤ìŒ', 'ì•„ë§ˆ',\n",
        "    '...', 'â€¦', 'â€œ', 'â€', 'â€˜', 'â€™', '-', '--', 'â€•',\n",
        "    'ê·¸ëƒ¥', 'ì§„ì§œ', 'ì™„ì „', 'ì•½ê°„', 'ì–´ë–»ê²Œ', 'ë­', 'ë”±', 'ë§‰', 'ë˜', 'ë˜ëŠ”',\n",
        "    'í•´ë„', 'ìƒê°', 'ì¤‘ëµ', 'ê°€ì§€', 'ì¡°ê¸ˆ', 'ë‹¤ë¬¸í™”', 'ì´ì œ',\n",
        "    'ì¤‘êµ­', 'ì¼ë³¸', 'í•œêµ­', 'ìš°ì¦ˆë² í‚¤ìŠ¤íƒ„', 'ë² íŠ¸ë‚¨', 'íƒœêµ­', 'ëª½ê³¨', 'ëª½ê³ ',\n",
        "    'ìê¸°', 'ê·¸ë•Œ', 'ê·¸ê±°', 'ìˆ˜ë„', 'ê·¸ê²Œ', 'ì—¬ëŸ¬', 'ë¬´ìŠ¨', 'ë„¤ë„¤', 'ë‚˜ì´', 'ì–´ë””',\n",
        "    'ë¨¼ì €', 'ëŒ€ë¶€ë¶„', 'ë‚˜ì¤‘', 'ëŒ€í•´', 'ê·¸ê²ƒ', 'ë­”ê°€', 'ì „í˜€', 'ì €í¬', 'ë§Œì•½', 'ì´ì£¼', 'ë‚˜ê°€ì•¼',\n",
        "   'ë‹¤ë¥¸', 'í•­ìƒ',  'ì–˜ê¸°', 'ë‚˜ë¼', 'ë¶€ë¶„', 'ì„ ìƒë‹˜'\n",
        "])\n",
        "standardization_dict = {\n",
        "      \"ì•„ë¹ \": \"ë‚¨í¸\", \"ì—¬ì\":\"ì—¬ì„±\", \"ì•„ê¸°\":\"ì•„ì´\",\n",
        "    \"í•œêµ­ë§\": \"í•œêµ­ì–´\", \"ì‹œì—„ë§ˆ\": \"ì‹œì–´ë¨¸ë‹ˆ\", \"ì–´ë¨¸ë‹ˆ\": \"ì‹œì–´ë¨¸ë‹ˆ\", \"ëŒ€í•œë¯¼êµ­\": \"í•œêµ­\",\n",
        "    \"ì™¸êµ­\": \"ì™¸êµ­ì¸\", \"ì™¸êµ­ì‚¬ëŒ\": \"ì™¸êµ­ì¸\",\n",
        "    \"ì°¨ë³„ì \": \"ì°¨ë³„\",\n",
        "    \"ë¬´ì‹œë‹¹í•¨\": \"ë¬´ì‹œ\", \"ë¬´ì‹œí•¨\": \"ë¬´ì‹œ\",\n",
        "    \"í¸ê²¬ë“¤\": \"í¸ê²¬\", \"íšŒì‚¬\":\"ì§ì¥\", \"ì—…ë¬´\":\"ì§ì¥\", \"ì¼\":\"ì§ì¥\",\"íšŒì‚¬ìƒí™œ\":\"ì§ì¥\",\n",
        "    \"ë§\": \"ì–¸ì–´\",\n",
        "    \"í•œêµ­ì‚¬ëŒ\": \"í•œêµ­ì¸\"\n",
        "}"
      ],
      "metadata": {
        "id": "Q248-72nNkM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_keywords(text):\n",
        "    words = text.split()\n",
        "    standardized_words = [standardization_dict.get(w, w) for w in words]\n",
        "\n",
        "    keywords = []\n",
        "    for word in standardized_words:\n",
        "        nouns = okt.nouns(word)\n",
        "        for noun in nouns:\n",
        "            if noun not in stopwords and len(noun) > 1:\n",
        "                keywords.append(noun)\n",
        "    return keywords"
      ],
      "metadata": {
        "id": "dlWyLejpNmTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "print(\"\\nğŸ“Œ ê° í† í”½ë³„ ìƒìœ„ í‚¤ì›Œë“œ (í˜•íƒœì†Œ + ë¶ˆìš©ì–´ ì œê±° + í‘œì¤€í™”):\")\n",
        "\n",
        "for topic_idx in range(7):\n",
        "    topic_docs = [\n",
        "        extract_keywords(text)\n",
        "        for i, text in enumerate(original_sentences)\n",
        "        if labels[i] == topic_idx\n",
        "    ]\n",
        "    all_words = [word for doc in topic_docs for word in doc]\n",
        "\n",
        "    if not all_words:\n",
        "        print(f\"\\nğŸ§© í† í”½ {topic_idx}: í‚¤ì›Œë“œ ì—†ìŒ\")\n",
        "        continue\n",
        "\n",
        "    top_words = Counter(all_words).most_common(10)\n",
        "    print(f\"\\nğŸ§© í† í”½ {topic_idx} í‚¤ì›Œë“œ:\")\n",
        "    for word, freq in top_words:\n",
        "        print(f\"- {word} ({freq})\")"
      ],
      "metadata": {
        "id": "KnU2YcCqNoUm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}